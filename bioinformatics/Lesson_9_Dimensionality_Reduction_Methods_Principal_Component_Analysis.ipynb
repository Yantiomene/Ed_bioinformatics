{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bde2b6a",
   "metadata": {
    "id": "5bde2b6a"
   },
   "source": [
    "<img src=\"materials/images/introduction-to-statistics-II-cover.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c03e64",
   "metadata": {
    "id": "f0c03e64"
   },
   "source": [
    "\n",
    "# üëã Welcome, before you start\n",
    "<br>\n",
    "\n",
    "### üìö Module overview\n",
    "\n",
    "We will go through eleven lessons with you:\n",
    "    \n",
    "- [**Lesson 1: Z-score**](Lesson_1_Z-score.ipynb)\n",
    "\n",
    "- [**Lesson 2: P-value**](Lesson_2_P-value.ipynb)\n",
    "\n",
    "- [**Lesson 3: Lesson 3: Welchs T-test**](Lesson_3_Welchs_T-test.ipynb)\n",
    "\n",
    "- [**Lesson 4: Log2 Fold Change**](Lesson_4_Log2_Fold_Change.ipynb)\n",
    "\n",
    "- [**Lesson 5: Pearson Correlation**](Lesson_5_Pearson_Correlation.ipynb)\n",
    "\n",
    "- [**Lesson 6: Spearman Correlation**](Lesson_6_Spearman_Correlation.ipynb)\n",
    "\n",
    "- [**Lesson 7: False Discovery Rate**](Lesson_7_False_Discovery_Rate.ipynb)\n",
    "\n",
    "- [**Lesson 8: Benjamini Hochberg**](Lesson_8_Benjamini_Hochberg.ipynb)\n",
    "\n",
    "- <font color=#E98300>**Lesson 9: Dimensionality Reduction Methods: Principal Component Analysis**</font>    `üìçYou are here.`\n",
    "\n",
    "- [**Lesson 10: Dimensionality Reduction Methods: t-SNE**](Lesson_10_Dimensionality_Reduction_Methods_t-SNE.ipynb)\n",
    "\n",
    "- [**Lesson 11: UMAP**](Lesson_11_UMAP.ipynb)\n",
    "</br>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>‚å®Ô∏è Keyboard shortcut</h3>\n",
    "\n",
    "These common shortcut could save your time going through this notebook:\n",
    "- Run the current cell: **`Enter + Shift`**.\n",
    "- Add a cell above the current cell: Press **`A`**.\n",
    "- Add a cell below the current cell: Press **`B`**.\n",
    "- Change a code cell to markdown cell: Select the cell, and then press **`M`**.\n",
    "- Delete a cell: Press **`D`** twice.\n",
    "\n",
    "Need more help with keyboard shortcut? Press **`H`** to look it up.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ba777",
   "metadata": {
    "id": "173ba777"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a1ec0",
   "metadata": {
    "id": "031a1ec0"
   },
   "source": [
    "# Lesson 9: Dimensionality Reduction Methods: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc03dd85",
   "metadata": {
    "id": "cc03dd85"
   },
   "source": [
    "<mark>**Principal Component Analysis (PCA)**</mark> is a method that is often used to reduce the **dimensionality (i.e. number of variables)** of large data sets. It works by transforming a large set of variables in to a smaller set that maintains most of the information of the larger set.\n",
    "\n",
    "Dimensionality reduction becomes useful often for the purposes of visualizing a dataset, or for making a dataset easier to work with, and faster for machine learning algorithms to process.\n",
    "\n",
    "In sum, the idea of PCA is to **reduce the number of variables** in a dataset, while **preserving as much information** as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8034a0f",
   "metadata": {
    "id": "e8034a0f"
   },
   "source": [
    "`üïí This module should take about 25 minutes to complete.`\n",
    "\n",
    "`‚úçÔ∏è This notebook is written using Python.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408feba",
   "metadata": {
    "id": "b408feba"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fcfafd",
   "metadata": {
    "id": "33fcfafd"
   },
   "source": [
    "## Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1e296",
   "metadata": {
    "id": "b4d1e296"
   },
   "source": [
    "**Principal components** are newly constructed variables that are efficient combinations of the original variables. The principal components represent the directions of the data that explain a maximal amount of variance, i.e, directions that capture the most information. \n",
    "\n",
    "For example, in the visualization below, you can see the data as displayed along the original x-axis and y-axis. However, the green line represents a nex axis where a large amount of variability exists among the data points. Additionally, perpendicular to that line, there also exists a large amount of variance among the data points. Those two axes (directions) could be two of the principal components since they contain a lot of information about the original variables. These new axes provide a better view of the differences among the variables. The larger the dispersion of the data points along a line, the larger the variance it carries and the more information it has.\n",
    "\n",
    "\n",
    "<img src=\"materials/images/images_dimensionality_reduction_principal_component_analysis/variability.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7cfef",
   "metadata": {
    "id": "33e7cfef"
   },
   "source": [
    "The newly constructed variables (i.e., principal components) below ensure that the information contained within one component does not overlap with the others. \n",
    "- The first principal component contains most of the information from the original variables. \n",
    "- The maximum remaining information is contained in the second component and so on for n (the number of original variables) components. \n",
    "\n",
    "Organizing information as principal components allows you to **reduce the dimensionality (i.e. number of variables)** of your dataset without losing much information **by discarding the components that contain minimal information**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6cb03",
   "metadata": {
    "id": "78b6cb03"
   },
   "source": [
    "<img src=\"materials/images/images_dimensionality_reduction_principal_component_analysis/principal_components_chart.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82d3f00",
   "metadata": {
    "id": "b82d3f00"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Note:</b> There are as many principal components as there are variables in the data set. The first principal component accounts for the largest possible variance in the dataset. The second principal component accounts for the next highest variance and is uncorrelated with (i.e., perpendicular) the first principal component. This continues until the total number of principal components is equal to the original number of variables.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66b94b",
   "metadata": {
    "id": "da66b94b"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7e954",
   "metadata": {
    "id": "28a7e954"
   },
   "source": [
    "# **Principal component analysis involves 5 steps:**\n",
    "    \n",
    "1. Standardize the range of values for each variable\n",
    "2. Compute the covariance matrix to identify correlations among the variables\n",
    "3. Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "4. Decide how many principal components to keep\n",
    "5. Reorient the data along the principal components' axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cef1d32",
   "metadata": {
    "id": "1cef1d32"
   },
   "source": [
    "## Step 1: Standardization\n",
    "\n",
    "$$\\large\\ standardization = \\frac{x-mean}{stdev}$$\n",
    "\n",
    "We first transform the variables to a comparable range of values by relating each data point to its respective mean, in terms of standard deviations. The aim of this step is to standardize the range of the variables so that each one contributes equally to the analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a5adf",
   "metadata": {
    "id": "ed3a5adf"
   },
   "source": [
    "## Step 2: Covariance matrix\n",
    "\n",
    "The purpose of this step is to see if there are any relationships among the variables in order to identify any redundant information. To identify these correlations, we compute the covariance matrix.\n",
    "\n",
    "The covariance matrix is a n √ó n symmetric matrix, where n is the number of variables. It contains the covariances associated with all possible pairs of the n variables.\n",
    "\n",
    "<img src=\"materials/images/images_dimensionality_reduction_principal_component_analysis/cov_matrix1.png\"/>\n",
    "\n",
    "It‚Äôs the signs of the covariances that tell us about the correlations between the variables. If the sign is positive, the two variables increase or decrease together (positively correlated). If the sign is negative, one variable increases when the other decreases (negatively correlated). There may also be times when there is little to no relationship between variables.\n",
    "\n",
    "<img src=\"materials/images/images_dimensionality_reduction_principal_component_analysis/cov_matrix2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b353b",
   "metadata": {
    "id": "0b3b353b"
   },
   "source": [
    "## Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix. \n",
    "\n",
    "Eigenvectors and eigenvalues are calculated from the covariance matrix, and always come in pairs such that every eigenvector has an eigenvalue. There are as many eigenvector/eigenvalue pairs as there are variables in the dataset. It is the eigenvectors and eigenvalues that determine the principal components of the data. \n",
    "\n",
    "The **eigenvectors** of the **covariance matrix** are actually the directions of the axes, where there is the most variance (most information); and it is these that we call <mark>**principal components**</mark>. \n",
    "\n",
    "The **eigenvalues** indicate the amount of variance carried in each eigenvector (i.e., principal component). By ranking the eigenvectors in order of their eigenvalues, highest to lowest, you get the <mark>principal components in order of importance</mark>. \n",
    "\n",
    "To determine the percentage of variance (information) accounted for by each principal component, we divide the eigenvalue of each component by the sum of eigenvalues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94cc48",
   "metadata": {
    "id": "da94cc48"
   },
   "source": [
    "## Step 4: Decide how many principal components to keep.\n",
    "\n",
    "Finally, you will decide whether to keep all of the principal components or discard those of lesser importance. \n",
    "\n",
    "If we decide to keep only the most important components (carrying the most information), then we are performing **dimensionality reduction**. Our final dataset will have only as many dimensions as the number of principal components we keep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0aac74",
   "metadata": {
    "id": "ba0aac74"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Alert:</b> The principal components are less interpretable than the original variables, and don‚Äôt have any real meaning since they are constructed as combinations of the original variables.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c270545e",
   "metadata": {
    "id": "c270545e"
   },
   "source": [
    "## Step 5: Re-orient the data along the principal components axes. \n",
    "Finally, we re-orient the data from the original axes to the ones represented by the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437452a",
   "metadata": {
    "id": "8437452a"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a865b3",
   "metadata": {
    "id": "f5a865b3"
   },
   "source": [
    "### PCA effectively provides:\n",
    "- A measure of how each variable is associated with the others - **Covariance matrix**.\n",
    "- Insight into the directions in which our data are dispersed (where most of the information resides)- **Eigenvectors**.\n",
    "- The relative importance of these different directions - **Eigenvalues**.\n",
    "- Dimensionality reduction of our data set by enabling us to drop the eigenvectors (i.e., principal components) that are relatively unimportant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187bb963",
   "metadata": {
    "id": "187bb963"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f6864",
   "metadata": {
    "id": "bf8f6864"
   },
   "source": [
    "# Example dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c09e2",
   "metadata": {
    "id": "6e9c09e2"
   },
   "source": [
    "<img src=\"materials/images/images_dimensionality_reduction_principal_component_analysis/cell_morphology.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f241d",
   "metadata": {
    "id": "4a5f241d"
   },
   "source": [
    "Below, we have a dataset of breast cancer cell morphology. We would like to use the variables to predict whether a tumor is benign or malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28b3eb",
   "metadata": {
    "id": "5e28b3eb"
   },
   "source": [
    "### ‚úÖ `Run` each of the cells below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b7450",
   "metadata": {
    "id": "f78b7450"
   },
   "source": [
    "### Preview the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf081c1",
   "metadata": {
    "id": "6cf081c1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/data_dimensionality_reduction_principal_component_analysis/breast-cancer.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef89a5",
   "metadata": {
    "id": "57ef89a5"
   },
   "source": [
    "There are 10 independent variables used to predict the dependent variable named \"diagnosis\". We will look to reduce the number of variables (dimensions) while maintaining predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976fb43",
   "metadata": {
    "id": "c976fb43"
   },
   "source": [
    "### Standardize the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05157340",
   "metadata": {
    "id": "05157340"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "# Split dataset into test/train sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3)\n",
    "\n",
    "# Standardize features\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4302809",
   "metadata": {
    "id": "e4302809"
   },
   "source": [
    "# Dimensionality Reduction\n",
    "We will perform principal component analysis to extract the 10 components with the most information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b505194",
   "metadata": {
    "id": "5b505194"
   },
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b271d",
   "metadata": {
    "id": "4b9b271d"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28816f36",
   "metadata": {
    "id": "28816f36"
   },
   "source": [
    "### Below is the explained variance ratios of the 10 principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863a138",
   "metadata": {
    "id": "f863a138"
   },
   "outputs": [],
   "source": [
    "ratios = pca.explained_variance_ratio_.round(3)\n",
    "pd.DataFrame(data = [ratios], \n",
    "             columns = [\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\", \"PC8\", \"PC9\", \"PC10\"], \n",
    "             index=[\"Variance Explained:\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edbcaa8",
   "metadata": {
    "id": "6edbcaa8"
   },
   "source": [
    "We can see that the first component is carrying about 56% of the information, more than any other components, justifying its position. \n",
    "\n",
    "The second component possesses around 24% of the information. The third component carries almost 9%; and the fourth component has just over 5% of the information. \n",
    "\n",
    "The remaining six components each possesses less than 5% of the information, and collectively carry only 6% of the information. \n",
    "\n",
    "We will look at the effects of reducing the dimensionality of the data set by removing these less important components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd87b6cb",
   "metadata": {
    "id": "bd87b6cb"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a78262",
   "metadata": {
    "id": "29a78262"
   },
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d888f7b6",
   "metadata": {
    "id": "d888f7b6"
   },
   "source": [
    "## Logistic Regression\n",
    "#### Perform logistic regression to make predictions using all 10 of the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490f506",
   "metadata": {
    "id": "0490f506"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_std, y_train)\n",
    "log_reg.score(X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21013620",
   "metadata": {
    "id": "21013620"
   },
   "source": [
    "#### Perform logistic regression to make predictions using all 10 of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d107afa",
   "metadata": {
    "id": "5d107afa"
   },
   "outputs": [],
   "source": [
    "# Use all 10 principal components\n",
    "log_reg.fit(X_train_pca, y_train)\n",
    "log_reg.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13ad2a9",
   "metadata": {
    "id": "b13ad2a9"
   },
   "source": [
    "We can see that the performance is identical when using all 10 of the original variables and all 10 of the principal components. They both achieved an accuracy of around <mark>**93%**</mark>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3ee51",
   "metadata": {
    "id": "ecf3ee51"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62344b5b",
   "metadata": {
    "id": "62344b5b"
   },
   "source": [
    "## Reduce the number of dimensions\n",
    "\n",
    "Let's reduce the number of dimensions to the top 5 principal components since they contain about 98% of the information.\n",
    "#### Perform logistic regression to make predictions using only the 5 most important principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b028eee",
   "metadata": {
    "id": "7b028eee"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "ratios = pca.explained_variance_ratio_.round(3)\n",
    "\n",
    "PC = pd.DataFrame(data = [ratios], \n",
    "             columns = [\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\"], \n",
    "             index=[\"Variance Explained:\"])\n",
    "PC[\"Total Variance\"] = PC.sum(axis=1)\n",
    "PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2f480",
   "metadata": {
    "id": "e3e2f480"
   },
   "outputs": [],
   "source": [
    "# Use 5 principal components\n",
    "log_reg.fit(X_train_pca, y_train)\n",
    "log_reg.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10bce5",
   "metadata": {
    "id": "1c10bce5"
   },
   "source": [
    "Note that even though we cut the number of dimensions in half (to 5), the performance remained almost identical at <mark>**92.3%**</mark>. This is because nearly all of the information is contained within those first 5 components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66371bb2",
   "metadata": {
    "id": "66371bb2"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9f0b8",
   "metadata": {
    "id": "95a9f0b8"
   },
   "source": [
    "## Reduce the number of dimensions again\n",
    "\n",
    "Let's reduce the number of dimensions one more time. Let's try the first 4 principal components which contain about 94% of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e015338",
   "metadata": {
    "id": "1e015338"
   },
   "source": [
    "#### Perform logistic regression to make predictions using only the 4 most important principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c5df60",
   "metadata": {
    "id": "78c5df60"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "ratios = pca.explained_variance_ratio_.round(3)\n",
    "\n",
    "PC = pd.DataFrame(data = [ratios], \n",
    "             columns = [\"PC1\", \"PC2\", \"PC3\", \"PC4\"], \n",
    "             index=[\"Variance Explained:\"])\n",
    "PC[\"Total Variance\"] = PC.sum(axis=1)\n",
    "PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7060b09",
   "metadata": {
    "id": "d7060b09"
   },
   "outputs": [],
   "source": [
    "# Use 4 principal components\n",
    "log_reg.fit(X_train_pca, y_train)\n",
    "log_reg.score(X_test_pca, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773571e",
   "metadata": {
    "id": "4773571e"
   },
   "source": [
    "## Accuracy using principal components and dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46013e96",
   "metadata": {
    "id": "46013e96"
   },
   "source": [
    "- 10 original variables: **92.9%**\n",
    "- 10 principal components: **92.9%**\n",
    "- 5 principal components **92.3%**\n",
    "- 4 principal component **91.2%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f464bbd9",
   "metadata": {
    "id": "f464bbd9"
   },
   "source": [
    "Reducing the dimensionality of the data set to 4 (from 10) dropped the performance only slightly to <mark>**91.2%**</mark> accuracy. This may be an excellent tradeoff--getting nearly identical performance while reducing the number of dimensions (variables) by 60%. **This is the advantage of using principal component analysis**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001556c0",
   "metadata": {
    "id": "001556c0"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231617a",
   "metadata": {
    "id": "9231617a"
   },
   "source": [
    "# üåü Ready for the next one?\n",
    "<br>\n",
    "\n",
    "\n",
    "- [**Lesson 10: Dimensionality Reduction Methods: t-SNE**](Lesson_10_Dimensionality_Reduction_Methods_t-SNE.ipynb)\n",
    "\n",
    "- [**Lesson 11: UMAP**](Lesson_11_UMAP.ipynb)\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16463c2",
   "metadata": {
    "id": "e16463c2"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe68e99",
   "metadata": {
    "id": "afe68e99"
   },
   "source": [
    "# Contributions & acknowledgment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190726d9",
   "metadata": {
    "id": "190726d9"
   },
   "source": [
    "Thanks Antony Ross for contributing the content for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a7461",
   "metadata": {
    "id": "2a6a7461"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971e31d1",
   "metadata": {
    "id": "971e31d1"
   },
   "source": [
    "Copyright (c) 2022 Stanford Data Ocean (SDO)\n",
    "\n",
    "All rights reserved."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
